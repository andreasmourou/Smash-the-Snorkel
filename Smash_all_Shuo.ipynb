{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](yelp2.jpg)\n",
    "# Smash the Snorkel Team Project: \n",
    "# A Weak Supervised Learning Study for Yelp Restaruant Review Classification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After manually labelling, our member realised that customers or visitors normally give a labelled \"1\" comments after they graded the business as a low grade (1 or 2 Star out of 5). Then, he did numerical study about the star and contribute couple of high coverage LFs to our Label Model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from snorkel.labeling import PandasLFApplier,LFAnalysis\n",
    "spark = SparkSession.builder.appName('pandasToSparkDF').getOrCreate()\n",
    "pd_dev = pd.read_csv(\"review_dev_labelled.csv\",header=0, index_col=0)\n",
    "df_dev = spark.createDataFrame(pd_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Source 1: Mongo DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Source 2: Local JOSN Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First:  restaurant\n",
    "biz_df = sqlContext.read.json(\"original_data/business.json\") # Mongo Needed\n",
    "biz_df.createOrReplaceTempView('biz_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data Spliting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Labeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Manual Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to train a classifier over the Yelp data that could predict whether a comment/text is positive (containing suggestions or complaints) or negative. Hence, we have access to a large amount of unlabelled data in the form of Yelp comments with some metadata. After filtered the dataset, we split the filtered review data into training set, testing set and development set. Because originally the dataset doesn't contain any labels, our members manually labelled the reviews in development and testing sets (500 labels for each) with \"1\" for positive and \"0\" for negative. Therefore, the result of our label model can be tested in a reasonable way later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Writing Labeling Functions 1: Text-based Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we used labelling functions (LFs) in Snorkel to apply the weak supervision approach by using programmatic rules and heuristics that assign labels to unlabelled training data. Individuals all have different methods to express their understanding and opinions, therefore, there is very unlikely exist completely same comments. Otherwise, if a single LF had high enough coverage to label our entire test dataset accurately, we don't need a classifier at all. In reality, most problems are not that simple. Instead, we usually need to combine multiple LFs to label our dataset, to improve the overall accuracy of the training labels we generate by factoring in multiple different signals, like keywords extraction, heuristics and numerical method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuo's Part:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> 3.2.1 Keywords Labeling Functions </b>\n",
    "\n",
    "<p>For text/comment in the dataset, some Labeling Functions(LFs later in this paper) could be written based on just keywords lookups. \n",
    "    These will often follow a realtive similar execution pattern, so we can create a template and use the resources parameter to pass\n",
    "    in specific keywords. Between, same for the labeling_function decorator, the LabelingFunction class wraps \n",
    "    a Python function (the f parameter), and we can use the resources parameter to pass in keyword arguments (here, our keywords to \n",
    "    lookup) to said function.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup for Labeling function\n",
    "import pyspark.sql.functions as F\n",
    "from snorkel.labeling import LabelModel\n",
    "from snorkel.labeling.apply.spark import SparkLFApplier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from snorkel.labeling import ,LFAnalysis\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from pyspark.sql import Row\n",
    "from snorkel.labeling.lf import labeling_function\n",
    "#from snorkel.labeling.lf.nlp_spark import spark_nlp_labeling_function\n",
    "from snorkel.preprocess import preprocessor\n",
    "\n",
    "ABSTAIN = -1\n",
    "NEGATIVE = 0\n",
    "POSITIVE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"NEGATIVE comments talk about 'I love pizza', 'Love this resturant', etc.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"NEGATIVE comments talk about 'This food is perfect', 'Perfect people' to describe.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"POSITIVE comments talk about 'The waiter is rude', 'Their behaviour is rude', etc.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.labeling import LabelingFunction\n",
    "\n",
    "def keyword_lookup(x, keywords, label):\n",
    "    if any(word in x.text.lower() for word in keywords):\n",
    "        return label\n",
    "    return ABSTAIN\n",
    "def make_keyword_lf(keywords, label=0):\n",
    "    return LabelingFunction(\n",
    "        name=f\"keyword_{keywords[0]}\",\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(keywords=keywords, label=label)\n",
    "    )\n",
    "\"\"\"NEGATIVE comments talk about 'I love pizza', 'Love this resturant', etc.\"\"\"\n",
    "keyword_love = make_keyword_lf(keywords=[\"love\"])\n",
    "\n",
    "\"\"\"NEGATIVE comments talk about 'This food is perfect', 'Perfect people' to describe.\"\"\"\n",
    "keyword_perfect = make_keyword_lf(keywords=[\"perfect\"])\n",
    "\n",
    "\"\"\"POSITIVE comments talk about 'The waiter is rude', 'Their behaviour is rude', etc.\"\"\"\n",
    "keyword_rude = make_keyword_lf(keywords=[\"rude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def keyword_less(x):\n",
    "    return POSITIVE if re.search(\"less\", x.text, flags=re.I) else ABSTAIN\n",
    "@labeling_function()\n",
    "def keyword_love(x):\n",
    "    return NEGATIVE if re.search(\"love\", x.text, flags=re.I) else ABSTAIN\n",
    "@labeling_function()\n",
    "def keyword_great(x):\n",
    "    return NEGATIVE if re.search(\"great\", x.text, flags=re.I) else ABSTAIN\n",
    "@labeling_function()\n",
    "def keyword_rude(x):\n",
    "    return POSITIVE if re.search(\"rude\", x.text, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> 3.2.2 Heuristic Keywords Labeling Functions </b>\n",
    "\n",
    "<p>When I manually labelled the development dataset at the very beginning stage,  \n",
    "    I realise the POSITIVE text always come up with high useful rate and vice versa.\n",
    "    Meanwhile, funny rate could \n",
    "    equal to 0, if this comment is labelled as 0. Hence, certain LFs were made.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def high_useful(x):\n",
    "    return POSITIVE if x.useful > 10 else ABSTAIN\n",
    "@labeling_function()\n",
    "def low_useful(x):\n",
    "    return NEGATIVE if x.useful < 2 else ABSTAIN\n",
    "@labeling_function()\n",
    "def funny(x):\n",
    "    return NEGATIVE if x.funny == 0 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> 3.2.2 Summary </b>\n",
    "\n",
    "<p>Overall, the coverage, accuracy and relationship among LFs made above can be obtained from below Label Metrics.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>keyword_love</th>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.032</td>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>0.742268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_great</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.038</td>\n",
       "      <td>109</td>\n",
       "      <td>62</td>\n",
       "      <td>0.637427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_rude</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_less</th>\n",
       "      <td>3</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.082</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low_useful</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.074</td>\n",
       "      <td>232</td>\n",
       "      <td>154</td>\n",
       "      <td>0.601036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high_useful</th>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>6</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.084</td>\n",
       "      <td>241</td>\n",
       "      <td>160</td>\n",
       "      <td>0.600998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               j Polarity  Coverage  Overlaps  Conflicts  Correct  Incorrect  \\\n",
       "keyword_love   0      [0]     0.194     0.188      0.032       72         25   \n",
       "keyword_great  1      [0]     0.342     0.316      0.038      109         62   \n",
       "keyword_rude   2      [1]     0.032     0.030      0.030       14          2   \n",
       "keyword_less   3      [1]     0.084     0.082      0.082       29         13   \n",
       "low_useful     4      [0]     0.772     0.722      0.074      232        154   \n",
       "high_useful    5      [1]     0.010     0.008      0.008        4          1   \n",
       "funny          6      [0]     0.802     0.760      0.084      241        160   \n",
       "\n",
       "               Emp. Acc.  \n",
       "keyword_love    0.742268  \n",
       "keyword_great   0.637427  \n",
       "keyword_rude    0.875000  \n",
       "keyword_less    0.690476  \n",
       "low_useful      0.601036  \n",
       "high_useful     0.800000  \n",
       "funny           0.600998  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "lfs = [keyword_love,keyword_great,keyword_rude,keyword_less,low_useful,high_useful, funny]\n",
    "applier = SparkLFApplier(lfs)\n",
    "L_dev = applier.apply(df_dev.rdd)\n",
    "g_label =np.array(df_dev.select('label').collect())\n",
    "LFAnalysis(L_dev, lfs).lf_summary(g_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pavlos's Part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simon's Part:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Writing Labeling Functions 2: Numerical Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jiaqi's Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Binary Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
