{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](yelp2.jpg)\n",
    "# Smash the Snorkel Team Project: \n",
    "# A Weak Supervised Learning Study for Yelp Restaruant Review Classification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Source 1: Mongo DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Source 2: Local JOSN Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First:  restaurant\n",
    "biz_df = sqlContext.read.json(\"original_data/business.json\") # Mongo Needed\n",
    "biz_df.createOrReplaceTempView('biz_table')\n",
    "#only keep restaurants\n",
    "biz_res = sqlContext.sql('SELECT * FROM biz_table WHERE categories LIKE \\'%Restaurants%\\'') # Mongo Needed\n",
    "biz_res.createOrReplaceTempView('biz_res_table')\n",
    "\n",
    "def filter_res(parent,filename,key): #filter the irrelevant records from other table\n",
    "    df = sqlContext.read.json(\"original_data/\"+filename)\n",
    "    table_name=filename[:-5]+'_table'\n",
    "    df.createOrReplaceTempView(filename[:-5]+'_table')\n",
    "    sqlquery = 'SELECT c.* FROM {} c LEFT JOIN {} b ON c.{} = b.{} WHERE b.{} IS NOT NULL'.format(table_name,parent,key,key,key)\n",
    "    print(sqlquery)\n",
    "    df_res=sqlContext.sql(sqlquery)\n",
    "    return df_res\n",
    "\n",
    "checkin_res = filter_res('biz_res_table','checkin.json','business_id')\n",
    "review_res = filter_res('biz_res_table','review.json','business_id')\n",
    "review_res.createOrReplaceTempView('review_res_table')\n",
    "tip_res = filter_res('review_res_table','tip.json','user_id') #since tip is on individual level, match them with users\n",
    "user_res = filter_res('review_res_table','user.json','user_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample business into traning_set dev_set and test_set\n",
    "import pyspark.sql.functions as f\n",
    "review_res = review_res.withColumn('index_1', f.monotonically_increasing_id())\n",
    "review_res.createOrReplaceTempView('review_res_table')\n",
    "sqlquery = 'SELECT * FROM review_res_table ORDER BY RAND(42) LIMIT {}'.format(1000)\n",
    "review_sample = sqlContext.sql(sqlquery)\n",
    "review_sample.createOrReplaceTempView('review_sample_table')\n",
    "\n",
    "sqlquery = 'SELECT a.* FROM review_res_table a LEFT JOIN review_sample_table b ON a.index_1 = b.index_1 WHERE b.index_1 IS NULL '\n",
    "review_train_all = sqlContext.sql(sqlquery)\n",
    "\n",
    "#sample 500 into dev_set and 500 into test_set\n",
    "from pyspark.sql.functions import desc\n",
    "review_sample = review_sample.withColumn('index_2', f.monotonically_increasing_id())\n",
    "review_dev = review_sample.limit(500)\n",
    "review_test = review_sample.sort(desc(\"index_2\")).limit(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from original training_set since it is too large for our single machine to apply the LFs\n",
    "review_train = review_train_all.limit(2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dev.toPandas().to_csv('filtered_data/review_dev.csv',header = 'true') #tospark\n",
    "review_test.toPandas().to_csv('filtered_data/review_test.csv',header = 'true') #tospark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Labeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Manual Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Writing Labeling Functions 1: Text-based Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuo's Part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pavlos's Part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simon's Part:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Writing Labeling Functions 2: Numerical Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jiaqi's Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Binary Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To be continue:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
