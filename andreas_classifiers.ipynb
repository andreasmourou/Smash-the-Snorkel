{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "import pandas as pd\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from snorkel.labeling import PandasLFApplier,LFAnalysis\n",
    "spark = SparkSession.builder.appName('pandasToSparkDF').getOrCreate()\n",
    "pd_dev = pd.read_csv(\"review_dev_labelled.csv\",header=0, index_col=0)\n",
    "df_dev = spark.createDataFrame(pd_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to apply Labeling function\n",
    "import pyspark.sql.functions as F\n",
    "from snorkel.labeling import LabelModel\n",
    "from snorkel.labeling.apply.spark import SparkLFApplier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from snorkel.labeling import ,LFAnalysis\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from pyspark.sql import Row\n",
    "from snorkel.labeling.lf import labeling_function\n",
    "#from snorkel.labeling.lf.nlp_spark import spark_nlp_labeling_function\n",
    "from snorkel.preprocess import preprocessor\n",
    "\n",
    "ABSTAIN = -1\n",
    "NEGATIVE = 0\n",
    "POSITIVE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+------+-----+\n",
      "|cool|funny|stars|useful|label|\n",
      "+----+-----+-----+------+-----+\n",
      "|   1|    0|    2|     0|    1|\n",
      "|   0|    0|    2|     0|    1|\n",
      "|   2|    0|    4|     4|    0|\n",
      "|   0|    0|    5|     0|    0|\n",
      "|   0|    0|    3|     0|    1|\n",
      "|   1|    0|    4|     2|    0|\n",
      "|   0|    0|    4|     1|    0|\n",
      "|   1|    1|    2|     2|    1|\n",
      "|   0|    0|    5|     3|    0|\n",
      "|   0|    0|    4|     0|    0|\n",
      "|   0|    0|    5|     0|    0|\n",
      "|   0|    0|    5|     0|    0|\n",
      "|   0|    0|    5|     0|    0|\n",
      "|   0|    0|    4|     0|    1|\n",
      "|   0|    0|    4|     0|    1|\n",
      "|   1|    1|    1|     1|    1|\n",
      "|   0|    0|    5|     0|    0|\n",
      "|   0|    0|    5|     0|    0|\n",
      "|   0|    0|    4|     1|    0|\n",
      "|   0|    0|    5|     1|    0|\n",
      "+----+-----+-----+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = df_dev.select(\"cool\", \"funny\", \"stars\", \"useful\", \"label\")\n",
    "cols = dataset.columns\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "categoricalColumns = [\"cool\", \"funny\", \"stars\", \"useful\"]\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    # encoder = OneHotEncoderEstimator(inputCol=categoricalCol + \"Index\", outputCol=categoricalCol + \"classVec\")\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol=\"label\", outputCol=\"label1\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all features into a vector using VectorAssembler\n",
    "#numericCols = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns]# + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "partialPipeline = Pipeline().setStages(stages)\n",
    "pipelineModel = partialPipeline.fit(dataset)\n",
    "preppedDataDF = pipelineModel.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedcols = [\"label1\", \"features\"] + cols\n",
    "dataset = preppedDataDF.select(selectedcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=123)\n",
    "print(trainingData.count())\n",
    "print(testData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label1\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "# LogisticRegression.transform() will only use the 'features' column.\n",
    "predictions = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label1: double, prediction: double, funny: bigint, stars: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected = predictions.select(\"label1\", \"prediction\", \"funny\", \"stars\")\n",
    "display(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8771006463527239\n",
      "areaUnderROC\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(evaluator.evaluate(predictions))\n",
    "print(evaluator.getMetricName())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the explanation of all the parameters in case we want to tune them\n",
    "#print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numNodes =  9\n",
      "depth =  3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_507e36d241f1) of depth 3 with 9 nodes"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create initial Decision Tree Model\n",
    "dt = DecisionTreeClassifier(labelCol=\"label1\", featuresCol=\"features\", maxDepth=3)\n",
    "\n",
    "# Train model with Training Data\n",
    "dtModel = dt.fit(trainingData)\n",
    "\n",
    "# Print details of the model\n",
    "print(\"numNodes = \", dtModel.numNodes)\n",
    "print(\"depth = \", dtModel.depth)\n",
    "display(dtModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC\n",
      "0.8995383194829177\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "predictions = dtModel.transform(testData)\n",
    "\n",
    "# Print the metric and the score\n",
    "print(evaluator.getMetricName())\n",
    "print(evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Hyperparameters of the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gini'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get default impurity measure\n",
    "dt.getImpurity()\n",
    "\n",
    "# This is in case we want to change to entropy and test it\n",
    "#dt.setImpurity(\"Entropy\")\n",
    "#dtModel = dt.fit(trainingData)\n",
    "#predictions = dtModel.transform(testData)\n",
    "#print(evaluator.getMetricName())\n",
    "#print(evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numNodes =  5\n",
      "depth =  2\n"
     ]
    }
   ],
   "source": [
    "# Create ParamGrid for Cross Validation\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth, [1, 2, 6, 10])\n",
    "             .addGrid(dt.maxBins, [20, 40, 80])\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "# Print the details of the best model\n",
    "print(\"numNodes = \", cvModel.bestModel.numNodes)\n",
    "print(\"depth = \", cvModel.bestModel.depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC\n",
      "0.897045244690674\n"
     ]
    }
   ],
   "source": [
    "# Use test set to measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(testData)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "print(evaluator.getMetricName())\n",
    "print(evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8927702004201821"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whats happening when you apply it on the training data\n",
    "pr = cvModel.transform(trainingData)\n",
    "\n",
    "evaluator.evaluate(pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create an initial RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC\n",
      "0.9005540166204986\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "predictions = rfModel.transform(testData)\n",
    "\n",
    "# Print the metric and the score\n",
    "print(evaluator.getMetricName())\n",
    "print(evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters of Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "             .addGrid(rf.maxBins, [20, 60])\n",
    "             .addGrid(rf.numTrees, [5, 20])\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.907202216066482"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(testData)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9248507686664224"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whats happening when you apply it on the training data\n",
    "pr = cvModel.transform(trainingData)\n",
    "\n",
    "evaluator.evaluate(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cool', 0.010644985107737726),\n",
       " ('funny', 0.025520850177026804),\n",
       " ('stars', 0.00419381054476834),\n",
       " ('useful', 0.01384711783173082),\n",
       " ('label', 0.008741266700017783)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Importances\n",
    "## WARNING! The columns are one-hot encoded therefore it does not make sense now\n",
    "list(zip(cols, rfModel.featureImportances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(36, {0: 0.0106, 1: 0.0255, 2: 0.0042, 3: 0.0138, 4: 0.0087, 5: 0.0026, 6: 0.0013, 7: 0.0007, 8: 0.0036, 10: 0.012, 11: 0.0108, 12: 0.0188, 13: 0.0057, 14: 0.0071, 15: 0.0013, 16: 0.0032, 17: 0.0055, 18: 0.0026, 19: 0.4271, 20: 0.056, 21: 0.0932, 22: 0.1832, 23: 0.0279, 24: 0.0158, 25: 0.0198, 26: 0.0006, 27: 0.0118, 28: 0.0018, 29: 0.0142, 30: 0.0101, 33: 0.0005})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
