{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](yelp2.jpg)\n",
    "# Smash the Snorkel Team Project: \n",
    "# A Weak Supervised Learning Study for Yelp Restaruant Review Classification \n",
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "## 1.1 Business Problem Identification\n",
    "\n",
    "The competition between restaurants is fiercer than ever. Restaurant owners are under consistent pressure \n",
    "to improve their business or set themselves apart from the rest. But how? Although some restuarants still rely \n",
    "on asking their customers directly, more and more restuarants have been leveraging reviews on apps like Yelp \n",
    "to understand what their customers need. In this case, the reviews containing complaints/suggestions are invaluable as they \n",
    "explicitly point out the possible solution to improving customers' satisfaction.\n",
    "\n",
    "For restuarants receiving numerous reviews,however, it is hard for the business owners to investigate every review to figure \n",
    "out whether the reviews contain useful suggestions/complaints. Therefore, an automatic process is needed to filter irrelevent \n",
    "information quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Project Objective\n",
    "In this project we seek to automize the classification of reviews to understand whether a review is mentioning complaints or suggestions using **Yelp** review dataset. One extra challenge that lies ahead is that we don't have any datapoint with the \"golden\" label indicating this piece of  information. \n",
    "Consequently, to cope with this problem, we aim to find solution to classification problem without any existing labels.\n",
    "\n",
    "**Objective**: Find solution for classification problem when there is no pre-exsiting label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Key Results\n",
    "There are four major results in this project.\n",
    "a) Build a Database hosting the data source to mimic the industrial canon. (Part 1)\n",
    "b) Create labeling functions using Snorkel DryBell to provide weak supervised-learning at industrial scale.\n",
    "c) Augment dataset to create more training datapoints\n",
    "d) Train a classifier for labeling a unseen review automatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Project Structure\n",
    "Below you can see a diagram that represents the workflow of the project. This includes the flow of the original files from start to end, and the actions taken in each step of the process. <br><br>\n",
    "As you can see we have acquired flat files in JSON format from the official yelp website. Then we have uploaded one of the files on the CosmosDB platform of Microsoft Azure in the MongoDB database that we have created. Due to the size of the files and the resource restrictions, we were not able to upload all of them. The flat files were then parsed into csv format in order to manipulate them as spark dataframes and we moved on to use the reviews dataframe as the basis of our analysis. This was split into 3 sets: <br>\n",
    "- The <b>development set</b> (500 rows) that was manually labeled and used to perform data understanding, analysis, aggregations feature engineering. This process also involved fetching the file from the MongoDB database to extract information. Then with this set we have created and evaluated the labeling functions. This whole process was iterative, as with each iteration we were imporoving the construction of the labeling functions.<br>\n",
    "-  The <b>train set</b> containing vast amount of data points was sampled to be able to apply efficiently the previously created labeling functions. This resulted to having a weakly labeled train set that was then augmented using data augmentation techniques like transformation functions. The output of this was the train set that we fit our classifiers in order to come to a final prediction.<br>\n",
    "- The <b>test set</b> (500 rows) that was manually labeled and used to validate and tune the classifiers.\n",
    "![Diagram.PNG](Diagram.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize our environment\n",
    "Before we move to the main part of our project (i.e. create labels for our existing datapoints), we are going to:\n",
    "- Give an overview of the packages that we are going to use\n",
    "- Load our dataset and have a brief look on it\n",
    "- Host a file of that dataset to a cloud-based Database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load the required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize properly pyspark dataframes\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pyspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Local Json Files \n",
    "For this project we are going to work extensively with the review.json file which contains reviews for different businesses from different users.\n",
    "\n",
    "In addition, we will host the business.json file in \n",
    "Due to the limit of database resource, we import the results from our database query\n",
    "In addition we import the .csv with the business_id's that they refer to restaurant\n",
    "\n",
    "\n",
    "but also the file with reviews from yelp.\n",
    "The json file is in a no-relational data storage format, therefore, before we can analyse our data we need to parse it into a more understandable format. \n",
    "Besides, some of the json files are quite large and hence loading them with pandas is going to be slow and make the local machine \n",
    "run out of memory easily. Accordingly, we utilise Pyspark to accelerate the computation.      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Host business.json file in cloud-based NoSQL DataBase\n",
    "\n",
    "In this step we demonstrate how a file from our dataset can be hosted in a cloud-based DataBase. We do this, as we would like to show how pyspark (package for distributed computing with Apache Spark) can operate with data extracted from a DataBase. \n",
    "\n",
    "\n",
    "### 2.3.1 Selecting provider and type of DataBase\n",
    "For the selection of our cloud provider but also for the type of our DataBase we have considered the following aspects (not in a strict rank):\n",
    "- The type of our files; all of them are .json files, a data format of semi-structured data\n",
    "- The available commercial DataBases that can work efficiently with .json files [MERGE WITH FIRST POINT]\n",
    "- The cloud provider which offer free credits for students\n",
    "- The documentantion of each provider for every database solution that they offer\n",
    "- The easy scalability as we did not know in advance what would be our real needs (especially in terms of throughput) for the succesful completion of our project\n",
    "\n",
    "Considering all this aspects we decided to proceed with the Cosmos DB from the Microsoft Azure.\n",
    "\n",
    "According to Azure's documentantion:\n",
    ">Azure Cosmos DB is Microsoft's globally distributed, multi-model database service. With a click of a button, Cosmos DB enables you to elastically and independently scale throughput and storage across any number of Azure regions worldwide. You can elastically scale throughput and storage, and take advantage of fast, single-digit-millisecond data access using your favorite API including: SQL, MongoDB, Cassandra, Tables, or Gremlin. \n",
    "\n",
    "Fom our project, we have used Cosmos DB as:\n",
    "- It has an extensive documentantion for users without any prior knowledge of setting-up a DataBase.\n",
    "- It is flexible not only in terms of resources, but also on the data type formats that it can store. \n",
    "- It provides a MongoDB API (one of the most common NoSQL databases) so we can operate it in the way that we should use a MongoDB database. The last point actually means more access to documentantion in the wild.\n",
    "\n",
    "\n",
    "### 2.3.2 Deploying the Database in Azure\n",
    "Below we demonstrate how we have created our DataBase but also how we have imported the business.json files from our dataset collection.\n",
    "\n",
    "#### 2.3.2.1 Create Azure Cosmos DB\n",
    "In the Azure portal we request to create a new Azure Cosmos DB. In the settings before deployment we select our API to be of MongoDB.\n",
    "\n",
    "\n",
    "![azure1.jpeg](./azure_screenshots/azure1.jpeg)\n",
    "\n",
    "#### 2.3.2.2 Access the Database from GUI (optional step)\n",
    "Below we demonstrate how we can access our created Database with Studio 3T\n",
    ">Studio 3T is a GUI and IDE for developers and data engineers who work with MongoDB. Data management features such as in-place editing and easy database connections are matched with polyglot query code generation, advanced shell with auto-completion, SQL import/export and enterprise level authentication with LDAP and Kerberos.\n",
    "\n",
    "Studio 3T give us access to the database from our local machine. With Studio 3T can manage remotely all of the aspects of the database. For example, we can create new collections in database (collection stores a set of different files), query the database but also see traffic in our database. In addition, it allows us to upload files to the database.\n",
    "\n",
    "To achieve this, we first copy the primary connection string from the properties of the database in Azure:\n",
    "\n",
    "![azure10.jpeg](./azure_screenshots/azure10.jpeg)\n",
    "\n",
    "Then we create a new connection in the Studio 3T and we add as uri, the primary connection string from Azure:\n",
    "\n",
    "![3T studio.png](./azure_screenshots/3TStudio.png)\n",
    "\n",
    "Now we can fully manage our database from our local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2.3 Importing the .json file to our database\n",
    "Normally, we could import our .json file with the use of 3T Studio. When we tried to do this we have received the error 16500 (TooManyRequests) from Azure. Our first approach was to increase the throughput to high levels (50000 RU/s). Although this approach has not resolved our problem.\n",
    "\n",
    "After some research we realized that this error occurs as we try to do a bulk import (import a big file to the database). To tackle this problem, we had to upload our file to an Azure storage account, and then transfer it from there to our database.\n",
    "\n",
    "Below we show how we have created a storage account in Azure where we uploaded our .json file:\n",
    "\n",
    "![azure2.jpeg](./azure_screenshots/azure2.jpeg)\n",
    "\n",
    "Finally, we have used the Azure Data Factory to create a temporary copy pipeline, which copied the file to the database:\n",
    "\n",
    "![azure3.jpeg](./azure_screenshots/azure3.jpeg)\n",
    "\n",
    "Azure Data Factory offer a visual interface to do this:\n",
    "\n",
    "![azure4.jpeg](./azure_screenshots/azure4.jpeg)\n",
    "\n",
    "For our problem, we've requested to \"copy data\".\n",
    "We selected the file from our storage account:\n",
    "\n",
    "![azure5.jpeg](./azure_screenshots/azure5.jpeg)\n",
    "\n",
    "And we used as destination our Cosmos DB:\n",
    "\n",
    "![azure6.jpeg](./azure_screenshots/azure6.jpeg)\n",
    "\n",
    "Below you can see the execution of the pipeline:\n",
    "\n",
    "![azure8.jpeg](./azure_screenshots/azure8.jpeg)\n",
    "\n",
    "After the succesful completion of the pipeline, we have finally managed to perform a bulk import into our database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2.4 Accessing our database through PySpark\n",
    "To execute queries without errors, we had first to increase the thoughput of our cosmos database.\n",
    "Below we increased the throughput from 400 RU/s (request units) to 10000. Note that by using a lower number of throughput actually leads to Too Many Requests error.\n",
    "\n",
    "![azure9.jpeg](./azure_screenshots/azure9.jpeg)\n",
    "\n",
    "The database with our business.json file is now ready to use. We are going to do the chapter 3 (Dataset Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Preprocessing\n",
    "In this part we are going to connect to our cloud-based DataBase to extract information about the different businesses.\n",
    "Based on this information we will make different subsets of our data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Connecting to the Cosmos DB (MongoDB API) with PySpark\n",
    "Below we demonstrate how we can connect to our DataBase with PySpark. In the .config(\"spark.mongodb.input.uri\") and .config(\"spark.mongodb.output.uri\" of Spark Session we used the primary connection string that Azure generated for the DataBase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.11:2.4.1\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"myApp\") \\\n",
    ".config(\"spark.mongodb.input.uri\", \"mongodb://smashsnorkel:qhMXOIygCLg5EsdGB2qEOQXCKwIBEWiKIA4bUvxhvrEqNQdwPA0YVw50iUNB75sAyb7eIN3ENmUDHCYoQ4sKUA==@smashsnorkel.mongo.cosmos.azure.com:10255/?ssl=true&replicaSet=globaldb&maxIdleTimeMS=120000&appName=@smashsnorkel@\") \\\n",
    ".config(\"spark.mongodb.input.readPreference.name\", \"secondaryPreferred\")\\\n",
    ".config(\"spark.mongodb.output.uri\", \"mongodb://smashsnorkel:qhMXOIygCLg5EsdGB2qEOQXCKwIBEWiKIA4bUvxhvrEqNQdwPA0YVw50iUNB75sAyb7eIN3ENmUDHCYoQ4sKUA==@smashsnorkel.mongo.cosmos.azure.com:10255/?ssl=true&replicaSet=globaldb&maxIdleTimeMS=120000&appName=@smashsnorkel@\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df01 = spark.read\\\n",
    "    .format(\"mongo\")\\\n",
    "    .option(\"database\",\"yelp\")\\\n",
    "    .option(\"collection\", \"business\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we perform a query in the database, where we request to get all business_ids, their average stars and their total reviews for the restaurant businesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col \n",
    "\n",
    "df01.createOrReplaceTempView(\"collection\")\n",
    "biz_res = sqlContext.sql('SELECT business_id, stars, review_count FROM collection WHERE categories LIKE  \\'%Restaurants%\\' ')\n",
    "biz_res.createOrReplaceTempView('biz_res_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For future reference in the part 2\n",
    "biz_res.toPandas().to_csv('filtered_data/biz_res.csv',header = 'true') #tospark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Filtering\n",
    "As we focus our classification task on the reviews for the restaurant, here we first filter the 'business' table to \n",
    "keep only the restaurants and based on this table we filter out the records in other table that is irrelevent to restaurants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#First:  restaurant\\nbiz_df = sqlContext.read.json(\"original_data/business.json\") # In our example we have used MongoDB\\nbiz_df.createOrReplaceTempView(\\'biz_table\\')\\n#only keep restaurants\\nbiz_res = sqlContext.sql(\\'SELECT * FROM biz_table WHERE categories LIKE \\'%Restaurants%\\'\\') # Mongo Needed\\nbiz_res.createOrReplaceTempView(\\'biz_res_table\\')\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT c.* FROM checkin_table c LEFT JOIN biz_res_table b ON c.business_id = b.business_id WHERE b.business_id IS NOT NULL\n",
      "SELECT c.* FROM review_table c LEFT JOIN biz_res_table b ON c.business_id = b.business_id WHERE b.business_id IS NOT NULL\n",
      "SELECT c.* FROM user_table c LEFT JOIN review_res_table b ON c.user_id = b.user_id WHERE b.user_id IS NOT NULL\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o89.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 12.0 failed 1 times, most recent failure: Lost task 13.0 in stage 12.0 (TID 543, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:456)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:456)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5c6420a2ab06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mcheckin_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckin_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filtered_data/checkin_res.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#tospark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#review_res = review_res.toPandas().to_csv('filtered_data/review_res.csv',header = 'true') #tospark - big file to execute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0muser_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filtered_data/user_res.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#tospark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2143\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \"\"\"\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o89.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 12.0 failed 1 times, most recent failure: Lost task 13.0 in stage 12.0 (TID 543, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:456)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:456)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "##This code actually executes the filtering by using the business.json from a local file\n",
    "'''#First:  restaurant\n",
    "biz_df = sqlContext.read.json(\"original_data/business.json\") # In our example we have used MongoDB\n",
    "biz_df.createOrReplaceTempView('biz_table')\n",
    "#only keep restaurants\n",
    "biz_res = sqlContext.sql('SELECT * FROM biz_table WHERE categories LIKE \\'%Restaurants%\\'') # Mongo Needed\n",
    "biz_res.createOrReplaceTempView('biz_res_table')\n",
    "'''\n",
    "######\n",
    "\n",
    "#filter the irrelevant records from other table\n",
    "def filter_res(parent,filename,key): \n",
    "    df = sqlContext.read.json(\"original_data/\"+filename)\n",
    "    table_name=filename[:-5]+'_table'\n",
    "    df.createOrReplaceTempView(filename[:-5]+'_table')\n",
    "    sqlquery = 'SELECT c.* FROM {} c LEFT JOIN {} b ON c.{} = b.{} WHERE b.{} IS NOT NULL'.format(table_name,parent,key,key,key)\n",
    "    print(sqlquery)\n",
    "    df_res=sqlContext.sql(sqlquery)\n",
    "    return df_res\n",
    "\n",
    "#keep only the relevant records\n",
    "checkin_res = filter_res('biz_res_table','checkin.json','business_id')\n",
    "review_res = filter_res('biz_res_table','review.json','business_id')\n",
    "review_res.createOrReplaceTempView('review_res_table')\n",
    "#tip_res = filter_res('review_res_table','tip.json','user_id') #since tip is on individual level, match them with users\n",
    "user_res = filter_res('review_res_table','user.json','user_id')\n",
    "\n",
    "#for future reference\n",
    "checkin_res = checkin_res.toPandas().to_csv('filtered_data/checkin_res.csv',header = 'true') #tospark\n",
    "#review_res = review_res.toPandas().to_csv('filtered_data/review_res.csv',header = 'true') #tospark - big file to execute\n",
    "user_res = user_res.toPandas().to_csv('filtered_data/user_res.csv',header = 'true') #tospark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Spliting\n",
    "Preparing for the development of labeling fuctions and the training of classifier, we split the filtered review data\n",
    "into three parts, namely training set, testing set and development set. We first randomly sample 1000 reviews out of all\n",
    "resturant reviews, and split the first 500 into development set and the rest into testing set. As we don't have labels at all,\n",
    "we will manually label the reviews in development and testing sets with 1 standing for suggestions/complaints and 0 for the rest.\n",
    "\n",
    "In addition, considering the computing resources we have it is not feasible for us to apply the labeling functions to all of the remaining\n",
    "6 million reviews. As a result, we sample 2500 reviews out of the \"complete\" training set focusing on demonstrating how the whole process may work \n",
    "when more computing resources are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample business into traning_set dev_set and test_set\n",
    "import pyspark.sql.functions as f\n",
    "review_res = review_res.withColumn('index_1', f.monotonically_increasing_id())\n",
    "review_res.createOrReplaceTempView('review_res_table')\n",
    "sqlquery = 'SELECT * FROM review_res_table ORDER BY RAND(42) LIMIT {}'.format(1000)\n",
    "review_sample = sqlContext.sql(sqlquery)\n",
    "review_sample.createOrReplaceTempView('review_sample_table')\n",
    "\n",
    "sqlquery = 'SELECT a.* FROM review_res_table a LEFT JOIN review_sample_table b ON a.index_1 = b.index_1 WHERE b.index_1 IS NULL '\n",
    "review_train_all = sqlContext.sql(sqlquery).cache()\n",
    "\n",
    "#sample 500 into dev_set and 500 into test_set\n",
    "from pyspark.sql.functions import desc\n",
    "review_sample = review_sample.withColumn('index_2', f.monotonically_increasing_id())\n",
    "review_dev = review_sample.limit(500).cache()\n",
    "review_test = review_sample.sort(desc(\"index_2\")).limit(500).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from original training_set since it is too large for our single machine to apply the LFs\n",
    "review_train_all.createOrReplaceTempView('review_train_all_table')\n",
    "sqlquery = 'SELECT * FROM review_train_all_table ORDER BY RAND(42) LIMIT {}'.format(2500)\n",
    "review_train = sqlContext.sql(sqlquery).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for future reference\n",
    "review_dev.toPandas().to_csv('filtered_data/review_dev.csv',header = 'true') #tospark\n",
    "review_test.toPandas().to_csv('filtered_data/review_test.csv',header = 'true') #tospark\n",
    "review_train.toPandas().to_csv('filtered_data/review_train.csv',header = 'true') #tospark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we have created three data frames. The development split (review_dev) will be used during the development phase of the Labelling functions. The test split (review_test) is our validation split. This will be used to evaluate the accuracy of the our labelling functions and will help us choose between the methods of assigning label probabilities. Then we will implement the chosen LFs on the train set (review_train) that will be then used to train our classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Manual Labeling\n",
    "Our goal is to train a classifier over the Yelp data that could predict whether a comment/text is positive (containing suggestions or complaints) or negative. Hence, we have access to a large amount of unlabelled data in the form of Yelp comments with some metadata. After filtered the dataset, we split the filtered review data into training set, testing set and development set. Because originally the dataset doesn't contain any labels, our members manually labelled the reviews in development and testing sets (500 labels for each) with \"1\" for positive and \"0\" for negative. Therefore, the result of our label model can be tested in a reasonable way later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 A connection with part 2\n",
    "At this stage we end with the pre-processing of our subsets. These subsets are going to be used in the part 2 of our analysis.\n",
    "We decided to split our work in two notebooks for three reasons:\n",
    "\n",
    "1. To keep seperate all the analysis regarding the creation and the use of the database with pyspark from the process of label creation\n",
    "2. To be able to run the rest of our analysis regardless of the available cloud resource; the database has been created for demonstration purposes and its use is limited due to the available credits. We have kept the analysis of label creation in part 2 as we might be interested to reproduce our results in the future.\n",
    "3. To deal with a problem that did not allow us to use different .jar files in faculty. The use of org.mongodb.spark:mongo-spark-connector_2.11:2.4.1 for making queries on the database, didn't allow us to use the sparknlp package (with its underlying jar files) on the same notebook and vice versa (as we couldn't use the .jar files to connect to the DataBase when we used the sparknlp package. It's an issue that could further be investigated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
