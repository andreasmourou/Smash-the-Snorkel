{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Hey I've added this so we can see properly SQL Views from Spark (Simon)\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "from pyspark.sql import SQLContext\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "import pandas as pd\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from snorkel.labeling import PandasLFApplier,LFAnalysis\n",
    "spark = SparkSession.builder.appName('pandasToSparkDF').getOrCreate()\n",
    "pd_dev = pd.read_csv(\"review_dev_labelled.csv\",header=0, index_col=0)\n",
    "df_dev = spark.createDataFrame(pd_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to apply Labeling function\n",
    "import pyspark.sql.functions as F\n",
    "from snorkel.labeling import LabelModel\n",
    "from snorkel.labeling.apply.spark import SparkLFApplier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from snorkel.labeling import ,LFAnalysis\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from pyspark.sql import Row\n",
    "from snorkel.labeling.lf import labeling_function\n",
    "#from snorkel.labeling.lf.nlp_spark import spark_nlp_labeling_function\n",
    "from snorkel.preprocess import preprocessor\n",
    "\n",
    "ABSTAIN = -1\n",
    "NEGATIVE = 0\n",
    "POSITIVE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LabelingFunction\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def short_comment(x):\n",
    "    \"\"\"Negative comments are often short, such as 'foods were insane'\"\"\"\n",
    "    return NEGATIVE if len(x.text.split()) < 20 else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def keyword_disgusting(x):\n",
    "    \"\"\"Negative comments are often short, such as 'foods were insane'\"\"\"\n",
    "    return POSITIVE if \"disgusting\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def keyword_cardboard(x):\n",
    "    \"\"\"Negative comments are often short, such as 'foods were insane'\"\"\"\n",
    "    return POSITIVE if \"cardboard\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def regex_home(x):\n",
    "    return POSITIVE if re.search(r\"home.*about\", x.text, flags=re.I) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def regex_regular(x):\n",
    "    return NEGATIVE if re.search(r\"regular*\", x.text, flags=re.I) else ABSTAIN\n",
    "@labeling_function()\n",
    "def regex_recomend(x):\n",
    "    return POSITIVE if re.search(r\"not.recommend*\", x.text, flags=re.I) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def Keyword_gem(x):\n",
    "    return NEGATIVE if \"gem\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def Keyword_perfect(x):\n",
    "    return NEGATIVE if \"perfect\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def Keyword_avoid(x):\n",
    "    return POSITIVE if \"avoid\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def regex_over_coocked(x):\n",
    "    return POSITIVE if re.search(r\"over.cooked*\", x.text, flags=re.I) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def Keyword_overpriced(x):\n",
    "    return POSITIVE if \"overpriced\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def long_comments(x):\n",
    "    \"\"\"Negative comments are often short, such as 'foods were insane'\"\"\"\n",
    "    return POSITIVE if len(x.text.split()) > 350 else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def key_good(x):\n",
    "    \"\"\"Negative comments are often short, such as 'foods were insane'\"\"\"\n",
    "    return POSITIVE if \"bad\" in x.text.lower() else ABSTAIN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [short_comment,long_comments,keyword_disgusting,keyword_cardboard,regex_home,\n",
    "       Keyword_perfect,Keyword_avoid,regex_over_coocked, Keyword_overpriced,key_good]\n",
    "applier = SparkLFApplier(lfs)\n",
    "L_dev = applier.apply(df_dev.rdd)\n",
    "g_label =np.array(df_dev.select('label').collect())\n",
    "x=LFAnalysis(L_dev, lfs).lf_summary(g_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: utils in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: utils-py in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (0.3.0)\n",
      "Requirement already satisfied: nose in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from utils-py) (1.3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install utils-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 193 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): en-core-web-sm==2.2.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz in /opt/anaconda/envs/Python3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: spacy>=2.2.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from en-core-web-sm==2.2.0) (2.2.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.0.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (45.2.0.post20200210)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.22.0)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (7.3.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.18.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.6.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->en-core-web-sm==2.2.0) (1.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (1.25.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (4.43.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda/envs/Python3/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->en-core-web-sm==2.2.0) (2.2.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.0-py3-none-any.whl size=12019121 sha256=a6e4c3ad618da670719e77d4eebea49a9cbaf0e49896c9d36ce615409bd4c163\n",
      "  Stored in directory: /home/faculty/.cache/pip/wheels/1d/bc/94/171b09b7fcce517723f40606754e5b7374770cc39290e092bf\n",
      "Successfully built en-core-web-sm\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/faculty/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from snorkel.augmentation import transformation_function\n",
    "import spacy\n",
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "import en_core_web_sm\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)\n",
    "\n",
    "import utils\n",
    "#from utils import preview_tfs\n",
    "from snorkel.augmentation import RandomPolicy\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define TFs\n",
    "\n",
    "def get_synonym(word, pos=None):\n",
    "    \"\"\"Get synonym for word given its part-of-speech (pos).\"\"\"\n",
    "    synsets = wn.synsets(word, pos=pos)\n",
    "    # Return None if wordnet has no synsets (synonym sets) for this word and pos.\n",
    "    if synsets:\n",
    "        words = [lemma.name() for lemma in synsets[0].lemmas()]\n",
    "        if words[0].lower() != word.lower():  # Skip if synonym is same as word.\n",
    "            # Multi word synonyms in wordnet use '_' as a separator e.g. reckon_with. Replace it with space.\n",
    "            return words[0].replace(\"_\", \" \")\n",
    "        \n",
    "def replace_token(spacy_doc, idx, replacement):\n",
    "    \"\"\"Replace token in position idx with replacement.\"\"\"\n",
    "    return \" \".join([spacy_doc[:idx].text, replacement, spacy_doc[1 + idx :].text])\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_verb_with_synonym(x):\n",
    "    # Get indices of verb tokens in sentence.\n",
    "    verb_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"VERB\"]\n",
    "    if verb_idxs:\n",
    "        # Pick random verb idx to replace.\n",
    "        idx = np.random.choice(verb_idxs)\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=\"v\")\n",
    "        # If there's a valid verb synonym, replace it. Otherwise, return None.\n",
    "        if synonym:\n",
    "            x.text = replace_token(x.doc, idx, synonym)\n",
    "            return x\n",
    "        \n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_noun_with_synonym(x):\n",
    "    # Get indices of noun tokens in sentence.\n",
    "    noun_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"NOUN\"]\n",
    "    if noun_idxs:\n",
    "        # Pick random noun idx to replace.\n",
    "        idx = np.random.choice(noun_idxs)\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=\"n\")\n",
    "        # If there's a valid noun synonym, replace it. Otherwise, return None.\n",
    "        if synonym:\n",
    "            x.text = replace_token(x.doc, idx, synonym)\n",
    "            return x\n",
    "\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_adjective_with_synonym(x):\n",
    "    # Get indices of adjective tokens in sentence.\n",
    "    adjective_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"ADJ\"]\n",
    "    if adjective_idxs:\n",
    "        # Pick random adjective idx to replace.\n",
    "        idx = np.random.choice(adjective_idxs)\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=\"a\")\n",
    "        # If there's a valid adjective synonym, replace it. Otherwise, return None.\n",
    "        if synonym:\n",
    "            x.text = replace_token(x.doc, idx, synonym)\n",
    "            return x\n",
    "@transformation_function(pre=[spacy])      \n",
    "def swap_adjectives(x):\n",
    "    adjective_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"ADJ\"]\n",
    "    # Check that there are at least two adjectives to swap.\n",
    "    if len(adjective_idxs) >= 2:\n",
    "        idx1, idx2 = sorted(np.random.choice(adjective_idxs, 2, replace=False))\n",
    "        # Swap tokens in positions idx1 and idx2.\n",
    "        x.text = \" \".join(\n",
    "            [\n",
    "                x.doc[:idx1].text,\n",
    "                x.doc[idx2].text,\n",
    "                x.doc[1 + idx1 : idx2].text,\n",
    "                x.doc[idx1].text,\n",
    "                x.doc[1 + idx2 :].text,\n",
    "            ]\n",
    "        )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs = [replace_verb_with_synonym,replace_noun_with_synonym,replace_adjective_with_synonym,swap_adjectives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from snorkel.augmentation import RandomPolicy\n",
    "random_policy = RandomPolicy(\n",
    "    len(tfs), sequence_length=2, n_per_original=2, keep_original=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.augmentation import PandasTFApplier\n",
    "tf_applier = PandasTFApplier(tfs,random_policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:25<00:00, 19.71it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train_augmented = tf_applier.apply(pd_dev)\n",
    "Y_train_augmented = df_train_augmented[\"label\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training set size: 6000\n",
      "Augmented training set size: 15312\n",
      "(500, 12)\n",
      "(1276, 12)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original training set size: {pd_dev.size}\")\n",
    "print(f\"Augmented training set size: {df_train_augmented.size}\")\n",
    "print(np.shape(pd_dev))\n",
    "print(np.shape(df_train_augmented))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I checked the rows and the replacement works.\n",
    "#df_train_augmented.tail()\n",
    "pd.options.display.max_rows = 999\n",
    "df_train_augmented.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
